# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O9D7DNAjOaALUDRDxhEeAWyyDc2w3-cX
"""




import pandas as pd
import numpy as np
from rdkit import Chem
from sklearn.model_selection import train_test_split
from collections import Counter
import time  # at top if not already imported
import os
from copy import deepcopy  # Add this with other imports at the top
import json
from datetime import datetime
from torch.utils.data.sampler import SubsetRandomSampler
from torch.utils.data import TensorDataset

import psutil
import torch.profiler
from contextlib import contextmanager
import tracemalloc


# Add these imports at the top of your file
import time
import psutil
import torch.profiler
from contextlib import contextmanager
import tracemalloc

# Add this class near the top after your imports
class PerformanceTracker:
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.total_training_time = 0.0
        self.total_epochs = 0
        self.total_samples = 0
        self.training_start_time = None
        self.flops_count = 0
        self.inference_times = []
        
    def start_training(self):
        self.training_start_time = time.time()
        
    def end_training(self):
        if self.training_start_time:
            self.total_training_time += time.time() - self.training_start_time
            self.training_start_time = None
    
    def add_epoch(self, num_samples):
        self.total_epochs += 1
        self.total_samples += num_samples
    
    def add_inference_time(self, inference_time_ms):
        self.inference_times.append(inference_time_ms)
    
    def get_throughput(self):
        if self.total_training_time > 0:
            return self.total_samples / self.total_training_time
        return 0.0
    
    def get_avg_inference_time(self):
        return sum(self.inference_times) / len(self.inference_times) if self.inference_times else 0.0

def count_flops(model, input_shape=(32, 128)):  # batch_size=32, feature_dim=128
    """Estimate FLOPs for the model"""
    total_flops = 0
    
    # Count ChemBERTa encoder FLOPs (approximation)
    # RoBERTa has ~125M parameters, roughly 250 GFLOPs per forward pass
    total_flops += 250e9  # ChemBERTa forward pass
    
    # Count other layers
    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Linear):
            # FLOPs = 2 * input_features * output_features * batch_size
            in_features = module.in_features
            out_features = module.out_features
            batch_size = input_shape[0]
            layer_flops = 2 * in_features * out_features * batch_size
            total_flops += layer_flops
    
    return total_flops / 1e6  # Return in millions



# ðŸ‘‡ Automatically detect all valid binary classification tasks from dataset
def get_valid_tasks(df, min_samples=100):
    """Identify valid binary classification tasks with sufficient samples"""
    valid_tasks = []
    
    # Use manually selected columns if specified
    if 'SELECTED_COLUMNS' in globals() and SELECTED_COLUMNS:
        columns_to_check = SELECTED_COLUMNS
        print(f"ðŸŽ¯ Using manually selected columns: {SELECTED_COLUMNS}")
    else:
        columns_to_check = [col for col in df.columns if col != "smiles"]
        print(f"ðŸ¤– Auto-detecting all columns")
    
    for col in columns_to_check:
        if col in df.columns and df[col].notna().sum() >= min_samples:
            unique_values = df[col].dropna().unique()
            if set(unique_values).issubset({0, 1}):
                valid_tasks.append(col)
    return valid_tasks

# âœ… Load uploaded CSV
SELECTED_COLUMNS = ["Label"]
df = pd.read_csv("sider.csv")

# Get all valid binary classification tasks
selected_tasks = get_valid_tasks(df)
print(f"Found {len(selected_tasks)} valid tasks: {selected_tasks}")


method = "cope"  # Now can be: "ewc","pass", "foster", "der", "il2a", "cope", "dmc", "bic", "lwf_mc", "memo"
'''=================================================='''

# âœ… Extract valid SMILES and labels for selected tasks
def extract_valid_smiles_labels(df, selected_tasks):
    """Extract valid SMILES and labels for selected tasks, handling each task independently"""
    smiles_dict = {}
    labels_dict = {}

    for task in selected_tasks:
        task_smiles = []
        task_labels = []
        for _, row in df.iterrows():
            mol = Chem.MolFromSmiles(row["smiles"])
            if mol and not np.isnan(row[task]):
                task_smiles.append(row["smiles"])
                task_labels.append(int(row[task]))  # ensure binary
        smiles_dict[task] = task_smiles
        labels_dict[task] = task_labels
    
    return smiles_dict, labels_dict

# Run extraction
smiles_dict, labels_dict = extract_valid_smiles_labels(df, selected_tasks)

# âœ… Split into ILR task structure
tasks = []
for task_name in selected_tasks:
    y = labels_dict[task_name]
    X = smiles_dict[task_name]

    # Skip tasks with insufficient data
    if len(X) < 100:  # Minimum samples per task
        print(f"Skipping task {task_name} - insufficient data ({len(X)} samples)")
        continue

    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42
    )

    tasks.append({
        'name': task_name,
        'train': (X_train, y_train),
        'val': (X_val, y_val),
        'class_balance': {0: y.count(0), 1: y.count(1)}
    })

# âœ… View class balance
for i, task in enumerate(tasks):
    print(f"Task {i+1} - {task['name']}")
    print("Train:", Counter(task['train'][1]))
    print("Val:", Counter(task['val'][1]))
    print()

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import RobertaModel, RobertaTokenizer
import numpy as np
import pandas as pd
from sklearn.metrics import f1_score, precision_recall_curve, auc, accuracy_score, roc_auc_score, confusion_matrix, classification_report
from sklearn.utils import resample
from rdkit import Chem
import matplotlib.pyplot as plt
from pprint import pprint
import warnings
warnings.filterwarnings('ignore')
torch.autograd.set_detect_anomaly(True)




class ChemBERTaEncoder(nn.Module):
    """Improved ChemBERTa encoder with multiple pooling strategies"""
    def __init__(self, output_dim=128):
        super().__init__()
        self.tokenizer = RobertaTokenizer.from_pretrained("seyonec/ChemBERTa-zinc-base-v1")
        self.encoder = RobertaModel.from_pretrained("seyonec/ChemBERTa-zinc-base-v1")
        
        # Multiple projection heads for different pooling strategies
        self.cls_projection = nn.Linear(self.encoder.config.hidden_size, output_dim)
        self.mean_projection = nn.Linear(self.encoder.config.hidden_size, output_dim)
        self.max_projection = nn.Linear(self.encoder.config.hidden_size, output_dim)
        
        # Attention pooling
        self.attention_weights = nn.Linear(self.encoder.config.hidden_size, 1)
        self.attention_projection = nn.Linear(self.encoder.config.hidden_size, output_dim)
        
        # Final fusion layer
        self.fusion = nn.Sequential(
            nn.Linear(output_dim * 4, output_dim * 2),
            nn.LayerNorm(output_dim * 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(output_dim * 2, output_dim)
        )
        
        # Unfreeze more layers for better adaptation
        for layer in self.encoder.encoder.layer[-8:]:  # Last 8 layers
            for param in layer.parameters():
                param.requires_grad = True

    def forward(self, smiles_list):
        if isinstance(smiles_list, str):
            smiles_list = [smiles_list]

        inputs = self.tokenizer(
            smiles_list, return_tensors="pt", padding=True, 
            truncation=True, max_length=512
        )
        
        device = next(self.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        outputs = self.encoder(**inputs)
        hidden_states = outputs.last_hidden_state
        
        # Multiple pooling strategies
        cls_emb = self.cls_projection(hidden_states[:, 0, :])
        mean_emb = self.mean_projection(hidden_states.mean(dim=1))
        max_emb = self.max_projection(hidden_states.max(dim=1)[0])
        
        # Attention pooling
        attn_weights = F.softmax(self.attention_weights(hidden_states), dim=1)
        attn_emb = self.attention_projection((attn_weights * hidden_states).sum(dim=1))
        
        # Fuse all representations
        fused = torch.cat([cls_emb, mean_emb, max_emb, attn_emb], dim=1)
        return self.fusion(fused)


    '''super().__init__()
    self.tokenizer = RobertaTokenizer.from_pretrained("seyonec/ChemBERTa-zinc-base-v1")
    self.encoder = RobertaModel.from_pretrained("seyonec/ChemBERTa-zinc-base-v1")
    self.projection = nn.Linear(self.encoder.config.hidden_size, output_dim)   
    # Freeze ChemBERTa parameters for stability
    for param in self.encoder.parameters():
        param.requires_grad = False'''

class SharedFeatureExtractor(nn.Module):
    """Shared Feature Extractor: ChemBERTa embeddings â†’ 64-D"""

    def __init__(self, input_dim=128, hidden_dim=96, output_dim=64, dropout=0.3):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),  # Changed from ReLU
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim),
            ResidualBlock(hidden_dim),  # New residual connection
            nn.Linear(hidden_dim, output_dim),
            nn.LayerNorm(output_dim)
        )

    def forward(self, x):
        return self.network(x)

class ResidualBlock(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.block = nn.Sequential(
            nn.Linear(dim, dim),
            nn.GELU(),
            nn.Linear(dim, dim)
        )
    def forward(self, x):
        return x + self.block(x)


class WeakFeatureExtractor(nn.Module):
    def __init__(self, input_dim=128, output_dim=32):
        super().__init__()
        # More complex architecture per paper
        self.network = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 48),  # Intermediate layer
            nn.LayerNorm(48),
            nn.ReLU(),
            nn.Linear(48, output_dim)
        )
        
    def forward(self, x):
        return self.network(x)

class CompressLayer(nn.Module):
    """Compress Layer: 64-D â†’ 32-D with gating"""

    def __init__(self, input_dim=64, output_dim=32):
        super().__init__()
        self.gate = nn.Sequential(
            nn.Linear(input_dim, input_dim),
            nn.Sigmoid()
        )
        self.compress = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        gate_weights = self.gate(x)
        gated_x = x * gate_weights
        compressed = self.compress(gated_x)
        return compressed


class CombineLayer(nn.Module):
    """Combine Layer: [Compressed shared + weak] â†’ Rectified Feature"""

    def __init__(self, input_dim=64, output_dim=64):
        super().__init__()
        self.combine = nn.Linear(input_dim, output_dim)

    def forward(self, compressed_shared, weak_features):
        combined = torch.cat([compressed_shared, weak_features], dim=1)
        rectified = self.combine(combined)
        return rectified


class HybridRectifierUnit(nn.Module):
    """Hybrid Rectifier Unit (activated from Task 2 onward)"""

    def __init__(self, chemberta_dim=128):
        super().__init__()
        self.weak_extractor = WeakFeatureExtractor(input_dim=chemberta_dim)
        self.compress_layer = CompressLayer()
        self.combine_layer = CombineLayer()

    def forward(self, chemberta_features, shared_features):
        # Weak feature extraction from ChemBERTa embeddings
        weak_features = self.weak_extractor(chemberta_features)

        # Compress shared features
        compressed_shared = self.compress_layer(shared_features)

        # Combine
        rectified_features = self.combine_layer(compressed_shared, weak_features)

        return rectified_features


class ClassifierHead(nn.Module):
    """Task-specific Classifier Head"""

    def __init__(self, input_dim=64, binary_output=True):
        super().__init__()
        self.binary_output = binary_output

        if binary_output:
            self.classifier = nn.Sequential(
                nn.Linear(input_dim, 32),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(32, 1)
            )
        else:
            self.classifier = nn.Sequential(
                nn.Linear(input_dim, 32),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(32, 2)
            )

    def forward(self, x):
        logits = self.classifier(x)
        logits = torch.nan_to_num(logits, nan=0.0, posinf=1e6, neginf=-1e6)
        return logits
class TaskVAE(nn.Module):
    def __init__(self, input_dim=64, latent_dim=16):
        super().__init__()
        self.latent_dim = latent_dim

        # Encoder to mu and logvar
        self.fc_mu = nn.Linear(input_dim, latent_dim)
        self.fc_logvar = nn.Linear(input_dim, latent_dim)

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 32),
            nn.ReLU(),
            nn.Linear(32, input_dim)
        )

    def encode(self, x):
        mu = self.fc_mu(x)
        logvar = self.fc_logvar(x)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon = self.decoder(z)
        return recon, mu, logvar



class AlignmentLoss(nn.Module):
    """Alignment Loss: L_align = Î± Â· L2 + Ï„ Â· Cosine"""

    def __init__(self, alpha=2.0, tau=1.0):
        super().__init__()
        self.alpha = alpha
        self.tau = tau
        self.cosine_loss = nn.CosineEmbeddingLoss()

    def forward(self, current_features, previous_features):
        min_batch_size = min(current_features.size(0), previous_features.size(0))
        current_features = current_features[:min_batch_size]
        previous_features = previous_features[:min_batch_size]

        # L2 loss
        l2_loss = F.mse_loss(current_features, previous_features)

        # Cosine loss
        batch_size = current_features.size(0)
        target = torch.ones(batch_size, device=current_features.device)
        cosine_loss = self.cosine_loss(current_features, previous_features, target)

        # Combined loss
        total_loss = self.alpha * l2_loss + self.tau * cosine_loss
        return total_loss
    
class Rectifier(nn.Module):
    def __init__(self, input_dim, compress_dim=384, weak_output_dim=128):
        super().__init__()
        self.ht = nn.Sequential(
            nn.Linear(200, 128),  # SMILES descriptor size = 200
            nn.ReLU(),
            nn.Linear(128, weak_output_dim)
        )
        self.at = nn.Linear(input_dim, compress_dim)
        self.bt = nn.Linear(compress_dim + weak_output_dim, input_dim)

    def forward(self, fx, x):
        hx = self.ht(x)
        compressed = self.at(fx)
        combined = torch.cat([compressed, hx], dim=1)
        return self.bt(combined)



class ILRModel(nn.Module):
    def to(self, device):
        super().to(device)
        self.device = device  # âœ… Set device properly
        self.chemberta_encoder.to(device)
        self.shared_extractor.to(device)
        self.hybrid_rectifier.to(device)
        self.classifier_heads.to(device)
        self.attention_updater.to(device)
        for head in self.foster_aux_heads.values():
            head.to(device)
        for param in self.pass_prototypes.values():
            param.to(device)
        return self

    def __init__(self, binary_output=True, chemberta_dim=128):
        super().__init__()
        self.chemberta_encoder = ChemBERTaEncoder(output_dim=chemberta_dim)
        self.shared_extractor = SharedFeatureExtractor(input_dim=chemberta_dim)
        self.hybrid_rectifier = HybridRectifierUnit(chemberta_dim=chemberta_dim)
        self.classifier_heads = nn.ModuleDict()
        self.alignment_loss = AlignmentLoss()

        self.previous_features = {}
        self.current_task = 1
        self.binary_output = binary_output
        self.chemberta_dim = chemberta_dim
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.feature_bank = {}
        self.pass_prototypes = nn.ParameterDict()
        self.pass_lambda = 1.0

        self.foster_aux_heads = nn.ModuleDict()
        self.foster_lambda = 1.0

        self.fetril_memory = {}
        self.fetril_temp = 0.1

        self.der_memory = {}
        self.der_alpha = 0.5

        self.attention_mask = None
        self.attention_updater = nn.Sequential(
            nn.Linear(64, 64),
            nn.Sigmoid()
        )
        self.il2a_lambda = 1.0

        self.lwfmc_temp = 2.0
        self.lwfmc_lambda = 1.0

        self.memo_memory = {}
        self.memo_lambda = 1.0
        self.memo_sample_size = 20
        self.rectifiers = nn.ModuleDict()  # Stores previous rectifiers
        self.generators = nn.ModuleDict()  # Stores VAE per task
        self.rectifiers = nn.ModuleDict()

    # [The rest of the class remains unchanged and continues below...]



    def add_task(self, task_id):
        """Add a new task-specific classifier head"""
        self.classifier_heads[str(task_id)] = ClassifierHead(
            input_dim=64, binary_output=self.binary_output
        )

    def forward(self, smiles_batch, task_id=None):
        device = next(self.parameters()).device
        if isinstance(smiles_batch, torch.Tensor):
            smiles_batch = smiles_batch.to(device)
        if task_id is None:
            task_id = self.current_task

        # 1. Extract ChemBERTa embeddings
        chemberta_features = self.chemberta_encoder(smiles_batch)
        chemberta_features = torch.nan_to_num(chemberta_features, nan=0.0, posinf=1e6, neginf=-1e6)

        # 2. Shared feature extraction
        shared_features = self.shared_extractor(chemberta_features)

        # 3. Rectifier chain processing
        if task_id == 1:
            # Base case - no rectification needed
            features_for_classification = shared_features
        else:
            # Initialize with current task's rectification
            current_features = self.hybrid_rectifier(chemberta_features, shared_features)
            
            # Backward pass through previous rectifiers (t-1, t-2, ..., 1)
            for t in range(task_id-1, 0, -1):
                if str(t) in self.rectifiers:
                    # Get stored rectifier components
                    rectifier = self.rectifiers[str(t)]
                    # Apply rectifier in sequence
                    current_features = rectifier(
                        chemberta_features,  # Original ChemBERTa features
                        current_features    # Features from previous rectifier
                    )
            
            features_for_classification = current_features

        # Ensure classifier exists and is on correct device
        if str(task_id) not in self.classifier_heads:
            self.add_task(task_id)
        self.classifier_heads[str(task_id)] = self.classifier_heads[str(task_id)].to(device)

        # Final classification
        predictions = self.classifier_heads[str(task_id)](features_for_classification)

        return predictions, features_for_classification, shared_features
    # Add to ILRModel class:
    def create_alignment_set(self, current_task_data, prev_task_data=None, size=256, use_generated=False):
        current_samples = resample(current_task_data, n_samples=size//2, replace=True)

        if prev_task_data and not use_generated:
            prev_samples = resample(prev_task_data, n_samples=size//2, replace=True)
            combined = current_samples + prev_samples
            return combined  # for non-generated case
        else:
            # Get ChemBERTa + Shared features
            chemberta_list = []
            shared_list = []
            label_list = []

            for smiles, label in current_samples:
                with torch.no_grad():
                    chemberta = self.chemberta_encoder([smiles]).squeeze(0).to(self.device)   # [128]
                    shared = self.shared_extractor(chemberta.unsqueeze(0)).squeeze(0)         # [64]
                chemberta_list.append(chemberta)
                shared_list.append(shared)
                label_list.append(label)

            # Convert lists to tensors
            chemberta_tensor = torch.stack(chemberta_list)
            shared_tensor = torch.stack(shared_list)
            labels_tensor = torch.tensor(label_list, dtype=torch.long).to(self.device)

            # Get generated features
            z = torch.randn(size//2, 16).to(self.device)
            generated_shared = self.generators[str(self.current_task - 1)].decoder(z)
            fake_chemberta = torch.zeros_like(chemberta_tensor[:size//2])  # dummy
            fake_labels = torch.randint(0, 2, (size//2,), device=self.device)

            # Combine real and generated
            all_chemberta = torch.cat([chemberta_tensor, fake_chemberta], dim=0)
            all_shared = torch.cat([shared_tensor, generated_shared], dim=0)
            all_labels = torch.cat([labels_tensor, fake_labels], dim=0)

            return TensorDataset(all_chemberta, all_shared, all_labels)

    def compute_alignment_loss(self, current_features, task_id):
        """Compute alignment loss with previous task features"""
        if task_id <= 1 or str(task_id-1) not in self.previous_features:
            return torch.tensor(0.0, device=current_features.device)

        previous_features = self.previous_features[str(task_id-1)]
        return self.alignment_loss(current_features, previous_features)

    def store_features(self, features, task_id):
        """Store features for alignment loss computation"""
        self.previous_features[str(task_id)] = features.detach().clone()

    def set_current_task(self, task_id):
        """Set the current task"""
        self.current_task = task_id

    def mtl_fecam_regularization(self, current_features, task_id):
        """MTL-FECAM regularization term"""
        if task_id == 1 or str(task_id-1) not in self.feature_bank:
            return 0.0
        
        prev_features = self.feature_bank[str(task_id-1)]
        attention_weights = self.attention(current_features)
        aligned_features = current_features * attention_weights
        return F.mse_loss(aligned_features, prev_features)

    def il2a_regularization(self, current_features):
        """IL2A regularization term"""
        if self.attention_mask is None:
            return 0.0
        
        current_attention = self.attention_updater(current_features)
        return F.mse_loss(current_attention, self.attention_mask)

    def update_feature_bank(self, features, task_id):
        """Update feature bank for MTL-FECAM"""
        self.feature_bank[str(task_id)] = features.detach().mean(dim=0, keepdim=True)

    def update_attention_mask(self, features):
        """Update attention mask for IL2A"""
        with torch.no_grad():
            new_mask = self.attention_updater(features).mean(dim=0)
            if self.attention_mask is None:
                self.attention_mask = new_mask
            else:
                # Exponential moving average
                self.attention_mask = 0.9 * self.attention_mask + 0.1 * new_mask
                
    def pass_regularization(self, features, task_id):
        """PASS: Prototype-Anchored Spatial Separation - FIXED"""
        if str(task_id) not in self.pass_prototypes:
            return torch.tensor(0.0, device=features.device)
        
        prototypes = self.pass_prototypes[str(task_id)]
        # Compute distance between current features and stored prototypes
        distance = F.mse_loss(features.mean(dim=0, keepdim=True), prototypes)
        return distance
    
    def foster_regularization(self, features, task_id):
        """FOSTER: Feature Boosting and Compression - FIXED"""
        if str(task_id) not in self.foster_aux_heads:
            return torch.tensor(0.0, device=features.device)
        
        aux_head = self.foster_aux_heads[str(task_id)]
        main_head = self.classifier_heads[str(task_id)]
        
        aux_pred = aux_head(features)
        main_pred = main_head(features)
        
        # Consistency loss between auxiliary and main predictions
        consistency_loss = F.mse_loss(aux_pred, main_pred.detach())
        return consistency_loss
    
    def fetril_regularization(self, features, task_id):
        """FeTril: Feature Translation for Incremental Learning - FIXED"""
        if task_id == 1 or str(task_id-1) not in self.fetril_memory:
            return torch.tensor(0.0, device=features.device)
        
        prev_features = self.fetril_memory[str(task_id-1)]
        
        # Ensure batch dimensions match
        min_batch = min(features.size(0), prev_features.size(0))
        current_batch = features[:min_batch]
        prev_batch = prev_features[:min_batch]
        
        # Cosine similarity loss
        sim_matrix = F.cosine_similarity(current_batch.unsqueeze(1), 
                                    prev_batch.unsqueeze(0), dim=2)
        return -sim_matrix.mean()  # Maximize similarity
    
    def der_regularization(self, task_id):
        """DER: Dark Experience Replay - FIXED"""
        if task_id == 1:
            return torch.tensor(0.0, device=next(self.parameters()).device)
        
        total_loss = torch.tensor(0.0, device=next(self.parameters()).device)
        count = 0
        
        for t in range(1, task_id):
            if str(t) in self.der_memory:
                old_logits, mem_features = self.der_memory[str(t)]
                # Get current predictions on old features
                current_logits = self.classifier_heads[str(t)](mem_features)
                total_loss += F.mse_loss(current_logits, old_logits)
                count += 1
        
        return total_loss / max(count, 1)
    
    def il2a_regularization(self, current_features):
        """IL2A: Keep existing implementation"""
        if self.attention_mask is None:
            return 0.0
        current_attention = self.attention_updater(current_features)
        return F.mse_loss(current_attention, self.attention_mask)
    
    def lwfmc_regularization(self, current_logits, old_logits):
        """LwF-MC regularization using knowledge distillation"""
        if old_logits is None:
            return torch.tensor(0.0, device=current_logits.device)
        
        # Soft targets with temperature
        old_probs = F.softmax(old_logits / self.lwfmc_temp, dim=1)
        current_log_probs = F.log_softmax(current_logits / self.lwfmc_temp, dim=1)
        
        return F.kl_div(current_log_probs, old_probs, reduction='batchmean') * (self.lwfmc_temp ** 2)

    def memo_regularization(self, current_features, task_id):
        """MEMO regularization using memory replay"""
        if task_id == 1 or str(task_id-1) not in self.memo_memory:
            return torch.tensor(0.0, device=current_features.device)
        
        # Get exemplars from previous task
        exemplars, exemplar_labels = self.memo_memory[str(task_id-1)]
        exemplars = exemplars.to(current_features.device)
        
        # Compute similarity between current features and exemplars
        sim_matrix = F.cosine_similarity(current_features.unsqueeze(1), 
                                       exemplars.unsqueeze(0), dim=2)
        
        # Maximize similarity to exemplars
        return -sim_matrix.mean()  # Negative because we want to maximize similarity
    
    # New update methods ======================================================
    
    def update_pass_prototypes(self, features, task_id):
        """Update PASS prototypes"""
        self.pass_prototypes[str(task_id)] = nn.Parameter(features.mean(dim=0, keepdim=True))
    
    def update_foster_heads(self, task_id):
        """Update FOSTER auxiliary heads"""
        self.foster_aux_heads[str(task_id)] = deepcopy(self.classifier_heads[str(task_id)])
    
    def update_fetril_memory(self, features, task_id):
        """Update FeTril memory bank"""
        self.fetril_memory[str(task_id)] = features.detach().clone()
    
    def update_der_memory(self, logits, features, task_id):
        """Update DER memory"""
        self.der_memory[str(task_id)] = (logits.detach(), features.detach())

    def bic_regularization(self, features, task_id):
        return (features * 0).sum() * 0.0
    
    def update_bic_calibration(self, features, task_id):
        """Update BIC calibration â€” placeholder"""
        # Typically stores calibration parameters (e.g. batch norm stats)
        pass

    def update_lwfmc_memory(self, logits, task_id):
        """Store logits for LwF-MC (not typically needed as we use previous model)"""
        pass

    def update_memo_memory(self, features, labels, task_id):
        """Update MEMO memory with exemplars"""
        # Select balanced exemplars for each class
        class_indices = {}
        for i, label in enumerate(labels):
            if label.item() not in class_indices:
                class_indices[label.item()] = []
            class_indices[label.item()].append(i)
        
        exemplars = []
        exemplar_labels = []
        
        for cls, indices in class_indices.items():
            # Sample exemplars for this class
            sample_size = min(self.memo_sample_size, len(indices))
            selected = np.random.choice(indices, size=sample_size, replace=False)
            
            exemplars.append(features[selected])
            exemplar_labels.append(labels[selected])
        
        if exemplars:
            self.memo_memory[str(task_id)] = (
                torch.cat(exemplars, dim=0),
                torch.cat(exemplar_labels, dim=0)
            )

    def oewc_penalty(self, oewc_data):
        """OEWC regularization term"""
        total = 0.0
        for name, param in self.named_parameters():
            if name in oewc_data["fisher"]:
                fisher = oewc_data["fisher"][name]
                prev_param = oewc_data["params"][name]
                total += (fisher * (param - prev_param).pow(2)).sum()
        return total

    def cope_regularization(self, current_features, task_id):
        """CoPE regularization placeholder"""
        return torch.tensor(0.0, device=current_features.device)

    def update_cope_memory(self, current_features, label, task_id):
        """Update CoPE memory"""
        pass

    def dmc_regularization(self, predictions, old_logits):
        """DMC distillation loss (MSE between current and old logits)"""
        return F.mse_loss(predictions, old_logits)
    
    
    def update_attention_mask(self, features):
        """Keep existing IL2A update"""
        with torch.no_grad():
            new_mask = self.attention_updater(features).mean(dim=0)
            if self.attention_mask is None:
                self.attention_mask = new_mask
            else:
                self.attention_mask = 0.9 * self.attention_mask + 0.1 * new_mask





class SMILESDataset(Dataset):
    """Dataset for molecular SMILES and toxicity labels"""

    def __init__(self, smiles_list, labels):
        self.smiles_list = smiles_list
        self.labels = labels

    def __len__(self):
        return len(self.smiles_list)

    def __getitem__(self, idx):
        return self.smiles_list[idx], torch.tensor(self.labels[idx])

class EWC:
    def __init__(self, model, dataloader, device='cpu'):
        self.model = model
        self.device = device
        self.dataloader = dataloader

        self.params = {n: p.clone().detach() for n, p in model.named_parameters() if p.requires_grad}
        self.fisher = self._compute_fisher()

    def _compute_fisher(self):
        fisher = {n: torch.zeros_like(p, device=self.device) for n, p in self.model.named_parameters() if p.requires_grad}
        self.model.eval()

        for batch in self.dataloader:
            batch_smiles, batch_labels = batch
            if isinstance(batch_smiles, torch.Tensor):
                batch_smiles = batch_smiles.to(self.device)
            batch_labels = batch_labels.float().to(self.device)

            self.model.zero_grad()
            preds, _, _ = self.model(batch_smiles)
            loss = nn.BCEWithLogitsLoss()(preds.squeeze(), batch_labels)
            loss.backward()

            for n, p in self.model.named_parameters():
                if p.grad is not None and n in fisher:
                    fisher[n] += (p.grad.detach() ** 2)

        for n in fisher:
            fisher[n] /= len(self.dataloader)

        return fisher

    def penalty(self, model):
        loss = 0.0
        for n, p in model.named_parameters():
            if n in self.params:
                loss += torch.sum(self.fisher[n] * (p - self.params[n]) ** 2)
        return loss


class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.bce = nn.BCEWithLogitsLoss(reduction='none')
        
    def forward(self, inputs, targets):
        bce_loss = self.bce(inputs, targets)
        pt = torch.exp(-bce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss
        return focal_loss.mean()


# âœ… Sample insertion of metric collection inside a function like train_task()
def finalize_training(self, start_time, task_id, eval_metrics):
    for eval_task_id in range(1, task_id + 1):
        eval_metrics = self.evaluate_task(self.task_val_loaders[eval_task_id], eval_task_id)

        if eval_task_id not in self.all_task_metrics:
            self.all_task_metrics[eval_task_id] = []
        self.all_task_metrics[eval_task_id].append(eval_metrics)

    # Calculate and print forgetting
    self.calculate_and_print_forgetting(task_id)

    # Final updates and cleanup
    end_time = time.time()
    print(f"ðŸ•“ Training Time: {end_time - start_time:.2f}s")

class ILRTrainer:
    """Training and evaluation class for ILR model"""       
    def __init__(self, model, method, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.method = method
        self.model = model.to(device)
        self.device = device
        self.performance_tracker = PerformanceTracker()
        self.model_flops = None
        
        # Initialize ALL method flags as False first
        self.ewc_enabled = False
        self.pass_enabled = False
        self.foster_enabled = False
        self.fetril_enabled = False
        self.der_enabled = False
        self.il2a_enabled = False
        self.bic_enabled = False
        self.lwfmc_enabled = False
        self.memo_enabled = False
        self.vanilla_enabled = False

        # Configure ONLY the selected method
        if self.method == "ewc":
            self.ewc_enabled = True
            self.ewc_list = []
            self.ewc_lambda = 100.0
            print("âœ… EWC method activated")
            
        elif self.method == "pass":
            self.pass_enabled = True
            self.pass_lambda = 1.0
            print("âœ… PASS method activated")
            
        elif self.method == "foster":
            self.foster_enabled = True
            self.foster_lambda = 1.0
            print("âœ… FOSTER method activated")
            
        elif self.method == "fetril":
            self.fetril_enabled = True
            self.fetril_lambda = 1.0
            print("âœ… FeTril method activated")
            
        elif self.method == "der":
            self.der_enabled = True
            self.der_lambda = 1.0
            print("âœ… DER method activated")
            
        elif self.method == "il2a":
            self.il2a_enabled = True
            self.il2a_lambda = 1.0
            print("âœ… IL2A method activated")
            
        elif self.method == "bic":
            self.bic_enabled = True
            self.bic_lambda = 1.0
            self.bias_correction_layers = {}
            print("âœ… BIC method activated")
            
        elif self.method == "lwf_mc":
            self.lwfmc_enabled = True
            self.lwfmc_lambda = 1.0
            self.old_model = None
            print("âœ… LwF-MC method activated")
            
        elif self.method == "memo":
            self.memo_enabled = True
            self.memo_lambda = 1.0
            print("âœ… MEMO method activated")
            
        elif self.method == "vanilla":
            self.vanilla_enabled = True
            print("âœ… Vanilla (no regularization) method activated")
            
        else:
            print(f"âŒ Unknown method: {self.method}. Using vanilla training.")
            self.vanilla_enabled = True
        
       # Initialize method-specific components
        self.ewc_list = [] if self.ewc_enabled else None
        self.ewc_lambda = 100.0 if self.ewc_enabled else 0.0
        self.pass_lambda = 1.0 if self.pass_enabled else 0.0
        self.foster_lambda = 1.0 if self.foster_enabled else 0.0
        self.fetril_lambda = 1.0 if self.fetril_enabled else 0.0
        self.der_lambda = 1.0 if self.der_enabled else 0.0
        self.il2a_lambda = 1.0 if self.il2a_enabled else 0.0
        self.bic_lambda = 1.0 if self.bic_enabled else 0.0
        self.lwfmc_lambda = 1.0 if self.lwfmc_enabled else 0.0
        self.memo_lambda = 1.0 if self.memo_enabled else 0.0

        # For LwF-MC
        self.old_model = None  # Stores previous model snapshot
        # âœ… New additions
        
        self.bias_correction_layers = {}  # For BIC bias heads

        self.oewc_enabled = (method == "oewc")
        self.oewc_lambda = 100.0
        self.oewc_fisher = None
        self.oewc_params = None

        self.cope_enabled = (method == "cope")
        self.cope_bank = {}   # task_id -> list of feature vectors
        self.cope_lambda = 1.0

        self.dmc_enabled = (method == "dmc")
        self.dmc_lambda = 1.0
        self.old_model = None  # For DMC frozen snapshot

        # Add these new attributes
        self.all_task_metrics = {task_id: [] for task_id in range(1, len(tasks)+1)}  # Stores metrics for all tasks over time
        self.task_val_loaders = {}  # Stores validation loaders for all tasks
        self.task_accuracies = {}

    def validate_method_setup(self):
        """Validate that only the selected method is active"""
        active_methods = []
        
        if self.ewc_enabled:
            active_methods.append("EWC")
        if self.pass_enabled:
            active_methods.append("PASS")
        if self.foster_enabled:
            active_methods.append("FOSTER")
        if self.fetril_enabled:
            active_methods.append("FeTril")
        if self.der_enabled:
            active_methods.append("DER")
        if self.il2a_enabled:
            active_methods.append("IL2A")
        if self.lwfmc_enabled:
            active_methods.append("LwF-MC")
        if self.memo_enabled:
            active_methods.append("MEMO")
        
        print(f"ðŸ” Active CL methods: {active_methods}")
        print(f"ðŸŽ¯ Selected method: {self.method.upper()}")
        
        if len(active_methods) != 1:
            print(f"âš ï¸  ERROR: Expected 1 active method, found {len(active_methods)}")
            return False
        return True
        
           



    def store_task_features(self, task_id, train_loader):
        """Store representative features for a completed task"""
        self.model.eval()
        all_features = []
        
        with torch.no_grad():
            for batch_smiles, _ in train_loader:
                if isinstance(batch_smiles, torch.Tensor):
                    batch_smiles = batch_smiles.to(self.device)
                
                _, features, _ = self.model(batch_smiles, task_id)
                all_features.append(features)
                
                # Only need a few batches for representative features
                if len(all_features) >= 3:
                    break
        
        if all_features:
            # Store mean features for this task
            mean_features = torch.cat(all_features, dim=0).mean(dim=0, keepdim=True)
            self.model.previous_features[str(task_id)] = mean_features
            print(f"âœ… Stored features for task {task_id}")

    def store_val_loader(self, task_id, val_loader):
        """Store validation loader for a task"""
        self.task_val_loaders[task_id] = val_loader    

    def calculate_forgetting(self):
        """Calculate forgetting metrics for all tasks"""
        forgetting = {}
        for task_id in self.all_task_metrics:
            if len(self.all_task_metrics[task_id]) > 1:
                # Get best accuracy during initial training
                best_acc = max([m['accuracy'] for m in self.all_task_metrics[task_id][:1]])
                # Get current accuracy
                current_acc = self.all_task_metrics[task_id][-1]['accuracy']
                forgetting[task_id] = best_acc - current_acc
            else:
                forgetting[task_id] = 0.0
        return forgetting
    
    def compute_cope_loss(self, current_features, labels, task_id):
        """
        Compute CoPE loss â€” placeholder.
        Replace with your actual loss logic if needed.
        """
        return torch.tensor(0.0, device=current_features.device)
    
    def verify_regularization_active(self, task_id, reg_loss):
        """Verify that regularization is actually being applied"""
        if task_id > 1 and reg_loss == 0.0:
            print(f"âš ï¸  WARNING: Task {task_id} but regularization loss is 0.0 for method {self.method}")
        elif reg_loss > 0.0:
            print(f"âœ… Regularization active: {self.method.upper()} loss = {reg_loss:.6f}")

    '''
    def train_task(self, train_loader, val_loader, val_loaders, task_id, epochs=10, lr=0.001):
        """Train model on a specific task"""

        print(f"\nðŸš€ Training Task {task_id} with {self.method.upper()} method...")
        # Validate setup
        if not self.validate_method_setup():
            print("âŒ Method setup validation failed!")
            return [], []
        self.performance_tracker.start_training()
        # Calculate FLOPs once if not done
        if self.model_flops is None:
            self.model_flops = count_flops(self.model)
        self.model.set_current_task(task_id)
        self.task_val_loaders[task_id] = val_loader
        self.store_val_loader(task_id, val_loader)
        

        if str(task_id) not in self.model.classifier_heads:
            self.model.add_task(task_id)
        self.model.classifier_heads[str(task_id)] = self.model.classifier_heads[str(task_id)].to(self.device)

        task_parameters = list(self.model.shared_extractor.parameters()) + \
                        list(self.model.hybrid_rectifier.parameters()) + \
                        list(self.model.classifier_heads[str(task_id)].parameters())
        # Optimizer setup
        optimizer = torch.optim.AdamW([
            {'params': self.model.shared_extractor.parameters(), 'lr': 1e-4},
            {'params': self.model.chemberta_encoder.parameters(), 'lr': 1e-5},
            {'params': self.model.classifier_heads[str(task_id)].parameters(), 'lr': 1e-3}
        ])
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

        y_train = train_loader.dataset.labels
        num_pos = sum(y_train)
        num_neg = len(y_train) - num_pos
        pos_weight_value = float(num_neg) / float(num_pos) if num_pos > 0 else 1.0
        pos_weight = torch.tensor([pos_weight_value], device=self.device)
        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

        best_val_f1 = 0.0
        best_accuracy = 0.0
        best_epoch = 0
        train_losses = []
        val_metrics = []

        start_time = time.time()

        for epoch in range(epochs):
            self.model.train()
            epoch_loss = 0.0
            epoch_samples = 0
            batch_count = 0

            for batch_smiles, batch_labels in train_loader:
                batch_labels = batch_labels.float().to(self.device)
                if isinstance(batch_smiles, torch.Tensor):
                    batch_smiles = batch_smiles.to(self.device)

                optimizer.zero_grad()

                predictions, current_features, shared_features = self.model(batch_smiles, task_id)
                class_loss = criterion(predictions.squeeze(), batch_labels)
                total_loss = class_loss
                regularization_loss = 0.0

                # Optional losses
                if self.method == "pass" and self.pass_enabled:
                # Only apply alignment loss for ILR-based methods
                    if task_id > 1:
                        align_loss = self.model.compute_alignment_loss(current_features, task_id)
                        regularization_loss += align_loss
                        
                    # PASS-specific regularization
                    pass_reg = self.model.pass_regularization(current_features, task_id)
                    regularization_loss += self.pass_lambda * pass_reg
                
                elif self.method == "ewc" and self.ewc_enabled:
                    if self.ewc_list:
                        ewc_penalty = sum(ewc.penalty(self.model) for ewc in self.ewc_list)
                        regularization_loss += self.ewc_lambda * ewc_penalty
                        print(f"EWC penalty: {ewc_penalty:.6f}")
                        self.verify_regularization_active(task_id, pass_reg)
                        
                elif self.method == "foster":
                    if task_id > 1:
                        align_loss = self.model.compute_alignment_loss(current_features, task_id)
                        regularization_loss += align_loss
                    foster_reg = self.model.foster_regularization(current_features, task_id)
                    regularization_loss += self.foster_lambda * foster_reg
                    print(f"FOSTER regularization: {foster_reg:.6f}")
                    self.verify_regularization_active(task_id, pass_reg)
                    
                elif self.method == "fetril":
                    if task_id > 1:
                        align_loss = self.model.compute_alignment_loss(current_features, task_id)
                        regularization_loss += align_loss
                    fetril_reg = self.model.fetril_regularization(current_features, task_id)
                    regularization_loss += self.fetril_lambda * fetril_reg
                    print(f"FeTril regularization: {fetril_reg:.6f}")
                    self.verify_regularization_active(task_id, pass_reg)
                    
                elif self.method == "der":
                    if task_id > 1:
                        align_loss = self.model.compute_alignment_loss(current_features, task_id)
                        regularization_loss += align_loss
                    der_reg = self.model.der_regularization(task_id)
                    regularization_loss += self.der_lambda * der_reg
                    print(f"DER regularization: {der_reg:.6f}")
                    self.verify_regularization_active(task_id, pass_reg)
                    
                elif self.method == "il2a":
                    if task_id > 1:
                        align_loss = self.model.compute_alignment_loss(current_features, task_id)
                        regularization_loss += align_loss
                    il2a_reg = self.model.il2a_regularization(current_features)
                    regularization_loss += self.il2a_lambda * il2a_reg
                    print(f"IL2A regularization: {il2a_reg:.6f}")
                    self.verify_regularization_active(task_id, pass_reg)
                    
                elif self.method == "lwf_mc":
                    if task_id > 1 and self.old_model is not None:
                        with torch.no_grad():
                            old_logits, _, _ = self.old_model(batch_smiles, task_id - 1)
                        lwfmc_loss = self.model.lwfmc_regularization(predictions, old_logits)
                        regularization_loss += self.lwfmc_lambda * lwfmc_loss
                        print(f"LwF-MC regularization: {lwfmc_loss:.6f}")
                        self.verify_regularization_active(task_id, pass_reg)
                        
                elif self.method == "memo":
                    if task_id > 1:
                        align_loss = self.model.compute_alignment_loss(current_features, task_id)
                        regularization_loss += align_loss
                    memo_loss = self.model.memo_regularization(current_features, task_id)
                    regularization_loss += self.memo_lambda * memo_loss
                    print(f"MEMO regularization: {memo_loss:.6f}")
                    self.verify_regularization_active(task_id, pass_reg)
                    
                elif self.method == "bic":
                    if task_id > 1:
                        align_loss = self.model.compute_alignment_loss(current_features, task_id)
                        regularization_loss += align_loss
                    bic_reg = self.model.bic_regularization(current_features, task_id)
                    regularization_loss += self.bic_lambda * bic_reg
                    print(f"BIC regularization: {bic_reg:.6f}")
                    self.verify_regularization_active(task_id, pass_reg)
                    
                elif self.method == "dmc":
                    if self.old_model is not None:
                        self.old_model.eval()
                        with torch.no_grad():
                            old_logits, _, _ = self.old_model(batch_smiles, task_id)
                        dmc_loss = self.model.dmc_regularization(predictions, old_logits)
                        regularization_loss += self.dmc_lambda * dmc_loss
                        print(f"DMC regularization: {dmc_loss:.6f}")
                        self.verify_regularization_active(task_id, pass_reg)
                        
                elif self.method == "oewc":
                    if self.oewc_fisher and self.oewc_params:
                        oewc_penalty = 0.0
                        for name, param in self.model.named_parameters():
                            if name in self.oewc_fisher:
                                oewc_penalty += torch.sum(self.oewc_fisher[name] * (param - self.oewc_params[name]).pow(2))
                        regularization_loss += self.oewc_lambda * oewc_penalty
                        print(f"OEWC penalty: {oewc_penalty:.6f}")
                        self.verify_regularization_active(task_id, pass_reg)
                        
                elif self.method == "cope":
                    cope_loss = self.compute_cope_loss(current_features, batch_labels, task_id)
                    regularization_loss += self.cope_lambda * cope_loss
                    print(f"CoPE regularization: {cope_loss:.6f}")
                    self.verify_regularization_active(task_id, pass_reg)
                
                # Add regularization to total loss
                total_loss += regularization_loss
                
                # Print loss breakdown every 10 batches
                if (batch_idx := getattr(self, '_batch_idx', 0)) % 10 == 0:
                    print(f"Epoch {epoch+1}, Batch {batch_idx}: Class Loss: {class_loss:.4f}, Reg Loss: {regularization_loss:.4f}")
                setattr(self, '_batch_idx', getattr(self, '_batch_idx', 0) + 1)

                total_loss.backward()
                optimizer.step()
                epoch_loss += total_loss.item()
                epoch_samples += batch_labels.size(0)
                batch_count += 1

            self.performance_tracker.add_epoch(epoch_samples)

            # Validation after epoch
            val_metrics_dict = self.evaluate_task(val_loader, threshold=0.5, task_id=task_id)
            val_f1 = val_metrics_dict['f1']
            acc = val_metrics_dict['accuracy']
            auroc = val_metrics_dict['auroc']
            auprc = val_metrics_dict['auc_pr']

            print(f"ðŸ“Š Epoch {epoch + 1} â€” Task {task_id} â€” Accuracy: {acc:.4f}, F1: {val_f1:.4f}, AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}")

            # Save best accuracy info
            if acc > best_accuracy:
                best_accuracy = acc
                best_epoch = epoch + 1

            train_losses.append(epoch_loss / len(train_loader))
            val_metrics.append(val_metrics_dict)
            scheduler.step()
        self.performance_tracker.end_training()

        # CRITICAL: Store features after task training completes
        print(f"ðŸ”§ Applying {self.method.upper()} post-training updates...")
        print(f"ðŸ’¾ Storing features for task {task_id}...")
        self.store_task_features(task_id, train_loader)

        # Method-specific updates after training
        with torch.no_grad():
            for batch_smiles, batch_labels in train_loader:
                batch_labels = batch_labels.float().to(self.device)
                if isinstance(batch_smiles, torch.Tensor):
                    batch_smiles = batch_smiles.to(self.device)
                
                predictions, current_features, shared_features = self.model(batch_smiles, task_id)

                if self.method == "ewc":
                    print("ðŸ“¦ Computing EWC Fisher Information...")
                    self.ewc_list.append(EWC(self.model, train_loader, device=self.device))
                    
                elif self.method == "pass":
                    # Update prototypes for PASS
                    with torch.no_grad():
                        all_features = []
                        for batch_smiles, _ in train_loader:
                            if isinstance(batch_smiles, torch.Tensor):
                                batch_smiles = batch_smiles.to(self.device)
                            _, features, _ = self.model(batch_smiles, task_id)
                            all_features.append(features)
                            if len(all_features) >= 3:  # Just need a few batches
                                break
                        if all_features:
                            mean_features = torch.cat(all_features, dim=0).mean(dim=0, keepdim=True)
                            self.model.update_pass_prototypes(mean_features, task_id)
                            print(f"âœ… Updated PASS prototypes for task {task_id}")
                            
                elif self.method == "foster":
                    self.model.update_foster_heads(task_id)
                    print(f"âœ… Updated FOSTER auxiliary heads for task {task_id}")
                    
                elif self.method == "fetril":
                    with torch.no_grad():
                        sample_batch = next(iter(train_loader))
                        batch_smiles, _ = sample_batch
                        if isinstance(batch_smiles, torch.Tensor):
                            batch_smiles = batch_smiles.to(self.device)
                        _, features, _ = self.model(batch_smiles, task_id)
                        self.model.update_fetril_memory(features, task_id)
                        print(f"âœ… Updated FeTril memory for task {task_id}")
                        
                elif self.method == "der":
                    with torch.no_grad():
                        sample_batch = next(iter(train_loader))
                        batch_smiles, _ = sample_batch
                        if isinstance(batch_smiles, torch.Tensor):
                            batch_smiles = batch_smiles.to(self.device)
                        predictions, features, _ = self.model(batch_smiles, task_id)
                        self.model.update_der_memory(predictions, features, task_id)
                        print(f"âœ… Updated DER memory for task {task_id}")
                        
                elif self.method == "il2a":
                    with torch.no_grad():
                        sample_batch = next(iter(train_loader))
                        batch_smiles, _ = sample_batch
                        if isinstance(batch_smiles, torch.Tensor):
                            batch_smiles = batch_smiles.to(self.device)
                        _, features, _ = self.model(batch_smiles, task_id)
                        self.model.update_attention_mask(features)
                        print(f"âœ… Updated IL2A attention mask for task {task_id}")
                        
                elif self.method == "lwf_mc":
                    # Store old model for next task
                    self.old_model = deepcopy(self.model)
                    self.old_model.eval()
                    for p in self.old_model.parameters():
                        p.requires_grad = False
                    print(f"âœ… Stored model snapshot for LwF-MC")
                    
                elif self.method == "memo":
                    with torch.no_grad():
                        sample_batch = next(iter(train_loader))
                        batch_smiles, batch_labels = sample_batch
                        if isinstance(batch_smiles, torch.Tensor):
                            batch_smiles = batch_smiles.to(self.device)
                        batch_labels = batch_labels.to(self.device)
                        _, features, _ = self.model(batch_smiles, task_id)
                        self.model.update_memo_memory(features, batch_labels, task_id)
                        print(f"âœ… Updated MEMO memory for task {task_id}")
                        
                elif self.method == "dmc":
                    # Store old model for distillation
                    self.old_model = deepcopy(self.model)
                    self.old_model.eval()
                    for p in self.old_model.parameters():
                        p.requires_grad = False
                    print(f"âœ… Stored model snapshot for DMC")
                    
                elif self.method == "oewc":
                    self.oewc_fisher, self.oewc_params = self.compute_oewc_fisher_and_params(train_loader)
                    print(f"âœ… Computed OEWC Fisher information and parameters")
                    
                elif self.method == "bic":
                    self.model.update_bic_calibration(None, task_id)  # Placeholder
                    print(f"âœ… Updated BIC calibration for task {task_id}")
                    
                elif self.method == "cope":
                    with torch.no_grad():
                        sample_batch = next(iter(train_loader))
                        batch_smiles, batch_labels = sample_batch
                        if isinstance(batch_smiles, torch.Tensor):
                            batch_smiles = batch_smiles.to(self.device)
                        batch_labels = batch_labels.to(self.device)
                        _, features, _ = self.model(batch_smiles, task_id)
                        self.model.update_cope_memory(features, batch_labels, task_id)
                        print(f"âœ… Updated CoPE memory for task {task_id}")

                break  # Only one batch needed

        # Train generator and rectifiers if task > 1
        if task_id > 1:
            print(f"\nðŸ” Starting generator + rectifier training for task {task_id}...\n")

            # 1. Extract previous task features
            prev_val_loader = val_loaders[task_id - 1]
            all_features = []
            self.model.eval()

            with torch.no_grad():
                for smiles_batch, _ in prev_val_loader:
                    if isinstance(smiles_batch, torch.Tensor):
                        smiles_batch = smiles_batch.to(self.device)

                    _, features, _ = self.model(smiles_batch, task_id - 1)
                    all_features.append(features)

            real_features = torch.cat(all_features, dim=0)

            # 2. Train generator (VAE) on these features
            self.train_generator(real_features, task_id - 1)

            # 3. Create hybrid alignment set (50% current real + 50% generated)
            align_set = self.model.create_alignment_set(
                current_task_data=train_loader.dataset,
                use_generated=True
            )
            align_loader = torch.utils.data.DataLoader(
                align_set,
                batch_size=32,
                shuffle=True
            )

            # 4. Train rectifiers using the hybrid set
            self.train_rectifiers(align_loader, task_id)

        # Compute EWC if needed
        if self.ewc_enabled:
            print("  ðŸ“¦ Computing EWC Fisher Information...")
            self.ewc_list.append(EWC(self.model, train_loader, device=self.device))
            
        elif self.pass_enabled:
            print("  ðŸŽ¯ Updating PASS prototypes...")
            self._update_pass_prototypes(train_loader, task_id)
            
        elif self.foster_enabled:
            print("  ðŸš€ Updating FOSTER auxiliary heads...")
            self.model.update_foster_heads(task_id)
            
        elif self.memo_enabled:
            print("  ðŸ§  Updating MEMO memory...")
            self._update_memo_memory(train_loader, task_id)
        
        elif self.lwfmc_enabled:
            print("  ðŸ“š Storing model snapshot for LwF-MC...")
            self.old_model = deepcopy(self.model)
            self.old_model.eval()
            for p in self.old_model.parameters():
                p.requires_grad = False

        if task_id > 1 and not self.vanilla_enabled:
            print(f"ðŸ” Training rectifiers for task {task_id}...")
            self._train_rectifiers_if_needed(train_loader, val_loaders, task_id)

        print(f"âœ… Best Accuracy for Task {task_id}: {best_accuracy:.4f} at Epoch {best_epoch}")
        end_time = time.time()
        print(f"ðŸ•“ Training Time: {end_time - start_time:.2f}s")

        return train_losses, val_metrics
    

    '''
    def train_task(self, train_loader, val_loader, val_loaders, task_id, epochs=10, lr=0.001):
        """COMPLETELY FIXED train_task method with proper regularization"""
        
        print(f"\nðŸš€ Training Task {task_id} with {self.method.upper()} method...")
        
        self.performance_tracker.start_training()
        
        # Calculate FLOPs once if not done
        if self.model_flops is None:
            self.model_flops = count_flops(self.model)
            
        self.model.set_current_task(task_id)
        self.task_val_loaders[task_id] = val_loader
        self.store_val_loader(task_id, val_loader)

        # Ensure classifier exists
        if str(task_id) not in self.model.classifier_heads:
            self.model.add_task(task_id)
        self.model.classifier_heads[str(task_id)] = self.model.classifier_heads[str(task_id)].to(self.device)

        # Optimizer setup
        task_parameters = list(self.model.shared_extractor.parameters()) + \
                        list(self.model.hybrid_rectifier.parameters()) + \
                        list(self.model.classifier_heads[str(task_id)].parameters())
        optimizer = torch.optim.AdamW([
            {'params': self.model.shared_extractor.parameters(), 'lr': 1e-4},
            {'params': self.model.chemberta_encoder.parameters(), 'lr': 1e-5},
        ])
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

        # Loss setup
        y_train = train_loader.dataset.labels
        num_pos = sum(y_train)
        num_neg = len(y_train) - num_pos
        pos_weight_value = float(num_neg) / float(num_pos) if num_pos > 0 else 1.0
        pos_weight = torch.tensor([pos_weight_value], device=self.device)
        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

        best_val_f1 = 0.0
        best_accuracy = 0.0
        best_epoch = 0
        train_losses = []
        val_metrics = []

        start_time = time.time()

        for epoch in range(epochs):
            self.model.train()
            epoch_loss = 0.0
            epoch_samples = 0
            batch_count = 0

            for batch_smiles, batch_labels in train_loader:
                batch_labels = batch_labels.float().to(self.device)
                if isinstance(batch_smiles, torch.Tensor):
                    batch_smiles = batch_smiles.to(self.device)

                optimizer.zero_grad()

                # Forward pass
                predictions, current_features, shared_features = self.model(batch_smiles, task_id)
                
                # Base classification loss
                class_loss = criterion(predictions.squeeze(), batch_labels)
                total_loss = class_loss
                regularization_loss = 0.0
                reg_loss_value = 0.0  # Initialize for tracking

                # ==========================================
                # APPLY METHOD-SPECIFIC REGULARIZATION
                # ==========================================
                
                if self.method == "ewc" and len(self.ewc_list) > 0:
                    ewc_penalty = sum(ewc.penalty(self.model) for ewc in self.ewc_list)
                    regularization_loss = self.ewc_lambda * ewc_penalty
                    reg_loss_value = ewc_penalty.item()
                    
                        
                elif self.method == "pass":
                    # ILR alignment loss for tasks > 1
                    if task_id > 1:
                        align_loss = self.model.compute_alignment_loss(current_features, task_id)
                        regularization_loss += align_loss
                    
                    # PASS-specific regularization
                    pass_reg = self.model.pass_regularization(current_features, task_id)
                    regularization_loss += self.pass_lambda * pass_reg
                    reg_loss_value = pass_reg.item()
                    
                        
                elif self.method == "foster":
                    if task_id > 1:
                        align_loss = self.model.compute_alignment_loss(current_features, task_id)
                        regularization_loss += align_loss
                    
                    foster_reg = self.model.foster_regularization(current_features, task_id)
                    regularization_loss += self.foster_lambda * foster_reg
                    reg_loss_value = foster_reg.item()
                    
                        
                elif self.method == "fetril":
                    if task_id > 1:
                        align_loss = self.model.compute_alignment_loss(current_features, task_id)
                        regularization_loss += align_loss
                    
                    fetril_reg = self.model.fetril_regularization(current_features, task_id)
                    regularization_loss += self.fetril_lambda * fetril_reg
                    reg_loss_value = fetril_reg.item()
                    
                        
                elif self.method == "der":
                    if task_id > 1:
                        align_loss = self.model.compute_alignment_loss(current_features, task_id)
                        regularization_loss += align_loss
                    
                    der_reg = self.model.der_regularization(task_id)
                    regularization_loss += self.der_lambda * der_reg
                    reg_loss_value = der_reg.item()
                    
                        
                elif self.method == "il2a":
                    if task_id > 1:
                        align_loss = self.model.compute_alignment_loss(current_features, task_id)
                        regularization_loss += align_loss
                    
                    il2a_reg = self.model.il2a_regularization(current_features)
                    regularization_loss += self.il2a_lambda * il2a_reg
                    reg_loss_value = il2a_reg if isinstance(il2a_reg, (int, float)) else il2a_reg.item()
                    
                        
                elif self.method == "bic":
                    if task_id > 1:
                        align_loss = self.model.compute_alignment_loss(current_features, task_id)
                        regularization_loss += align_loss
                    
                    bic_reg = self.model.bic_regularization(current_features, task_id)
                    regularization_loss += self.bic_lambda * bic_reg
                    reg_loss_value = bic_reg.item()
                    
                        
                elif self.method == "lwf_mc":
                    if task_id > 1 and self.old_model is not None:
                        with torch.no_grad():
                            old_logits, _, _ = self.old_model(batch_smiles, task_id - 1)
                        lwfmc_loss = self.model.lwfmc_regularization(predictions, old_logits)
                        regularization_loss += self.lwfmc_lambda * lwfmc_loss
                        reg_loss_value = lwfmc_loss.item()
                        

                            
                elif self.method == "memo":
                    if task_id > 1:
                        align_loss = self.model.compute_alignment_loss(current_features, task_id)
                        regularization_loss += align_loss
                    
                    memo_loss = self.model.memo_regularization(current_features, task_id)
                    regularization_loss += self.memo_lambda * memo_loss
                    reg_loss_value = memo_loss.item()

                        
                elif self.method == "dmc":
                    if self.old_model is not None:
                        self.old_model.eval()
                        with torch.no_grad():
                            old_logits, _, _ = self.old_model(batch_smiles, task_id)
                        dmc_loss = self.model.dmc_regularization(predictions, old_logits)
                        regularization_loss += self.dmc_lambda * dmc_loss
                        reg_loss_value = dmc_loss.item()
                        

                            
                elif self.method == "oewc":
                    if self.oewc_fisher and self.oewc_params:
                        oewc_penalty = 0.0
                        for name, param in self.model.named_parameters():
                            if name in self.oewc_fisher:
                                oewc_penalty += torch.sum(self.oewc_fisher[name] * (param - self.oewc_params[name]).pow(2))
                        regularization_loss += self.oewc_lambda * oewc_penalty
                        reg_loss_value = oewc_penalty.item()
                        
                            
                elif self.method == "cope":
                    cope_loss = self.compute_cope_loss(current_features, batch_labels, task_id)
                    regularization_loss += self.cope_lambda * cope_loss
                    reg_loss_value = cope_loss.item()
                    
                
                # Add regularization to total loss
                total_loss += regularization_loss
                
                # Verify regularization is working
                if task_id > 1 and reg_loss_value == 0.0:
                    print(f"  âš ï¸  WARNING: Task {task_id} but regularization loss is 0.0 for method {self.method}")
                elif reg_loss_value > 0.0 and batch_count % 50 == 0:
                    print(f"  âœ… Regularization active: {self.method.upper()} loss = {reg_loss_value:.6f}")
                
                # Print loss breakdown every 50 batches
                if batch_count % 50 == 0:
                    print(f"    Epoch {epoch+1}, Batch {batch_count}: "
                        f"Class Loss: {class_loss:.4f}, "
                        f"Reg Loss: {regularization_loss:.4f}, "
                        f"Total: {total_loss:.4f}")

                # Backward pass
                total_loss.backward()
                optimizer.step()
                epoch_loss += total_loss.item()
                epoch_samples += batch_labels.size(0)
                batch_count += 1

            self.performance_tracker.add_epoch(epoch_samples)

            # Validation after epoch
            val_metrics_dict = self.evaluate_task(val_loader, threshold=0.5, task_id=task_id)
            val_f1 = val_metrics_dict['f1']
            acc = val_metrics_dict['accuracy']
            auroc = val_metrics_dict['auroc']
            auprc = val_metrics_dict['auc_pr']

            print(f"  ðŸ“Š Epoch {epoch + 1}/{epochs} â€” "
                f"Loss: {epoch_loss/batch_count:.4f}, "
                f"Acc: {acc:.4f}, F1: {val_f1:.4f}, "
                f"AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}")

            # Save best accuracy info
            if acc > best_accuracy:
                best_accuracy = acc
                best_epoch = epoch + 1

            train_losses.append(epoch_loss / len(train_loader))
            val_metrics.append(val_metrics_dict)
            scheduler.step()
            
        self.performance_tracker.end_training()

        # ==========================================
        # POST-TRAINING METHOD-SPECIFIC UPDATES
        # ==========================================
        
        print(f"ðŸ”§ Applying {self.method.upper()} post-training updates...")
        print(f"ðŸ’¾ Storing features for task {task_id}...")
        self.store_task_features(task_id, train_loader)

        # Method-specific updates after training
        with torch.no_grad():
            sample_batch = next(iter(train_loader))
            batch_smiles, batch_labels = sample_batch
            batch_labels = batch_labels.float().to(self.device)
            if isinstance(batch_smiles, torch.Tensor):
                batch_smiles = batch_smiles.to(self.device)
            
            predictions, current_features, shared_features = self.model(batch_smiles, task_id)

            if self.method == "ewc":
                print("  ðŸ“¦ Computing EWC Fisher Information...")
                self.ewc_list.append(EWC(self.model, train_loader, device=self.device))
                
            elif self.method == "pass":
                print("  ðŸŽ¯ Updating PASS prototypes...")
                all_features = []
                for batch_smiles, _ in train_loader:
                    if isinstance(batch_smiles, torch.Tensor):
                        batch_smiles = batch_smiles.to(self.device)
                    _, features, _ = self.model(batch_smiles, task_id)
                    all_features.append(features)
                    if len(all_features) >= 3:
                        break
                if all_features:
                    mean_features = torch.cat(all_features, dim=0).mean(dim=0, keepdim=True)
                    self.model.update_pass_prototypes(mean_features, task_id)
                    print(f"    âœ… Updated PASS prototypes for task {task_id}")
                    
            elif self.method == "foster":
                self.model.update_foster_heads(task_id)
                print(f"    âœ… Updated FOSTER auxiliary heads for task {task_id}")
                
            elif self.method == "fetril":
                self.model.update_fetril_memory(current_features, task_id)
                print(f"    âœ… Updated FeTril memory for task {task_id}")
                
            elif self.method == "der":
                self.model.update_der_memory(predictions, current_features, task_id)
                print(f"    âœ… Updated DER memory for task {task_id}")
                
            elif self.method == "il2a":
                self.model.update_attention_mask(current_features)
                print(f"    âœ… Updated IL2A attention mask for task {task_id}")
                
            elif self.method == "lwf_mc":
                # Store old model for next task
                self.old_model = deepcopy(self.model)
                self.old_model.eval()
                for p in self.old_model.parameters():
                    p.requires_grad = False
                print(f"    âœ… Stored model snapshot for LwF-MC")
                
            elif self.method == "memo":
                self.model.update_memo_memory(current_features, batch_labels, task_id)
                print(f"    âœ… Updated MEMO memory for task {task_id}")
                
            elif self.method == "dmc":
                # Store old model for distillation
                self.old_model = deepcopy(self.model)
                self.old_model.eval()
                for p in self.old_model.parameters():
                    p.requires_grad = False
                print(f"    âœ… Stored model snapshot for DMC")
                
            elif self.method == "oewc":
                self.oewc_fisher, self.oewc_params = self.compute_oewc_fisher_and_params(train_loader)
                print(f"    âœ… Computed OEWC Fisher information and parameters")
                
            elif self.method == "bic":
                self.model.update_bic_calibration(None, task_id)
                print(f"    âœ… Updated BIC calibration for task {task_id}")
                
            elif self.method == "cope":
                self.model.update_cope_memory(current_features, batch_labels, task_id)
                print(f"    âœ… Updated CoPE memory for task {task_id}")

        # Train generator and rectifiers if task > 1
        if task_id > 1:
            print(f"\nðŸ” Starting generator + rectifier training for task {task_id}...\n")

            # 1. Extract previous task features
            prev_val_loader = val_loaders[task_id - 1]
            all_features = []
            self.model.eval()

            with torch.no_grad():
                for smiles_batch, _ in prev_val_loader:
                    if isinstance(smiles_batch, torch.Tensor):
                        smiles_batch = smiles_batch.to(self.device)

                    _, features, _ = self.model(smiles_batch, task_id - 1)
                    all_features.append(features)

            real_features = torch.cat(all_features, dim=0)

            # 2. Train generator (VAE) on these features
            self.train_generator(real_features, task_id - 1)

            # 3. Create hybrid alignment set (50% current real + 50% generated)
            align_set = self.model.create_alignment_set(
                current_task_data=train_loader.dataset,
                use_generated=True
            )
            align_loader = torch.utils.data.DataLoader(
                align_set,
                batch_size=32,
                shuffle=True
            )

            # 4. Train rectifiers using the hybrid set
            self.train_rectifiers(align_loader, task_id)

        print(f"âœ… Task {task_id} training completed. Best Accuracy: {best_accuracy:.4f} at Epoch {best_epoch}")
        end_time = time.time()
        print(f"ðŸ•“ Training Time: {end_time - start_time:.2f}s")

        return train_losses, val_metrics

    def _update_pass_prototypes(self, train_loader, task_id):
        """Update PASS prototypes"""
        with torch.no_grad():
            all_features = []
            for batch_smiles, _ in train_loader:
                if isinstance(batch_smiles, torch.Tensor):
                    batch_smiles = batch_smiles.to(self.device)
                _, features, _ = self.model(batch_smiles, task_id)
                all_features.append(features)
                if len(all_features) >= 3:  # Just need a few batches
                    break
            if all_features:
                mean_features = torch.cat(all_features, dim=0).mean(dim=0, keepdim=True)
                self.model.update_pass_prototypes(mean_features, task_id)

    def _update_memo_memory(self, train_loader, task_id):
        """Update MEMO memory"""
        with torch.no_grad():
            sample_batch = next(iter(train_loader))
            batch_smiles, batch_labels = sample_batch
            if isinstance(batch_smiles, torch.Tensor):
                batch_smiles = batch_smiles.to(self.device)
            batch_labels = batch_labels.to(self.device)
            _, features, _ = self.model(batch_smiles, task_id)
            self.model.update_memo_memory(features, batch_labels, task_id)

    def _train_rectifiers_if_needed(self, train_loader, val_loaders, task_id):
        """Train rectifiers if needed"""
        if hasattr(self, 'train_generator') and hasattr(self, 'train_rectifiers'):
            # Extract previous task features
            prev_val_loader = val_loaders[task_id - 1]
            all_features = []
            self.model.eval()

            with torch.no_grad():
                for smiles_batch, _ in prev_val_loader:
                    if isinstance(smiles_batch, torch.Tensor):
                        smiles_batch = smiles_batch.to(self.device)
                    _, features, _ = self.model(smiles_batch, task_id - 1)
                    all_features.append(features)

            if all_features:
                real_features = torch.cat(all_features, dim=0)
                self.train_generator(real_features, task_id - 1)

                # Create alignment set and train rectifiers
                align_set = self.model.create_alignment_set(
                    current_task_data=train_loader.dataset,
                    use_generated=True
                )
                align_loader = torch.utils.data.DataLoader(
                    align_set, batch_size=32, shuffle=True
                )
                self.train_rectifiers(align_loader, task_id)

    def train_generator(self, real_features, task_id, epochs=50):
        """Full VAE training with KL-divergence"""
        vae = TaskVAE(input_dim=real_features.size(1)).to(self.device)
        optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)
        
        # Dataset and loader
        dataset = torch.utils.data.TensorDataset(real_features)
        loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)
        
        for epoch in range(epochs):
            for batch in loader:
                x = batch[0].to(self.device)
                
                # Forward pass
                recon, mu, logvar = vae(x)
                
                # Losses
                recon_loss = F.mse_loss(recon, x, reduction='sum')
                kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
                
                total_loss = recon_loss + 0.1 * kl_div  # Î²-VAE with Î²=0.1
                
                # Backward pass
                optimizer.zero_grad()
                total_loss.backward()
                optimizer.step()
            
            if epoch % 10 == 0:
                print(f'Generator Epoch {epoch}, Loss: {total_loss.item()/len(x):.4f}')
        
        # Store trained generator
        self.model.generators[str(task_id)] = vae
        print(f'âœ… Saved generator for task {task_id}')
        
        # Visual check
        with torch.no_grad():
            z = torch.randn(5, vae.latent_dim).to(self.device)
            samples = vae.decoder(z)
            print(f'Sample features mean: {samples.mean().item():.3f}, std: {samples.std().item():.3f}')

    # ðŸ”¥ ADD THIS NEW METHOD AFTER train_task():
    def train_rectifiers(self, alignment_loader, task_id, epochs=10):
        """Train rectifier components after main task training"""
        if task_id <= 1:
            return  # No rectifiers for first task

        # Check if previous task features exist
        prev_task_key = str(task_id - 1)
        if prev_task_key not in self.model.previous_features:
            print(f"âš ï¸  Warning: No stored features for task {task_id - 1}. Skipping rectifier training.")
            return

        # Only train rectifier components
        rectifier_params = list(self.model.hybrid_rectifier.parameters())
        optimizer = torch.optim.Adam(rectifier_params, lr=1e-4)

        print(f"ðŸ”§ Training rectifiers for task {task_id}...")

        for epoch in range(epochs):
            self.model.train()
            epoch_loss = 0
            batch_count = 0

            try:
                for chemberta_features, shared_features, _ in alignment_loader:
                    chemberta_features = chemberta_features.to(self.device)  # [B, 128]
                    shared_features = shared_features.to(self.device)        # [B, 64]

                    optimizer.zero_grad()

                    # Forward pass through hybrid rectifier
                    rectified = self.model.hybrid_rectifier(
                        chemberta_features, shared_features
                    )

                    # Get previous task features
                    prev_features = self.model.previous_features[prev_task_key]
                    
                    # Handle batch size mismatch
                    min_batch_size = min(rectified.size(0), prev_features.size(0))
                    if min_batch_size > 0:
                        rectified_batch = rectified[:min_batch_size]
                        prev_batch = prev_features[:min_batch_size]
                        
                        # Compute alignment loss
                        align_loss = F.mse_loss(rectified_batch, prev_batch)
                        
                        align_loss.backward()
                        optimizer.step()
                        epoch_loss += align_loss.item()
                        batch_count += 1

                if batch_count > 0:
                    avg_loss = epoch_loss / batch_count
                    print(f"  Rectifier Epoch {epoch+1} - Loss: {avg_loss:.4f}")
                else:
                    print(f"  Rectifier Epoch {epoch+1} - No valid batches processed")
                    
            except Exception as e:
                print(f"  âš ï¸  Error in rectifier training epoch {epoch+1}: {str(e)}")
                continue

        print(f"âœ… Completed rectifier training for task {task_id}")

    def evaluate_task(self, test_loader, task_id, threshold=0.5):
        """Evaluate model on a specific task"""
        self.model.eval()
        all_predictions = []
        all_labels = []
        all_probs = []
        inference_times = []
        start_eval_time = time.time()
        total_eval_samples = 0

        with torch.no_grad():
            for batch_smiles, batch_labels in test_loader:
                batch_labels = batch_labels.to(self.device)
                if isinstance(batch_smiles, torch.Tensor):
                    batch_smiles = batch_smiles.to(self.device)
                
                batch_start = time.time()
                predictions, _, _ = self.model(batch_smiles, task_id)
                batch_end = time.time()
                batch_inference_time = (batch_end - batch_start) * 1000  # Convert to ms
                inference_times.append(batch_inference_time)
                probs = torch.sigmoid(predictions.squeeze()).detach().cpu().numpy()
                preds = (probs > threshold).astype(int)

                all_predictions.extend(preds)
                all_labels.extend(batch_labels.cpu().numpy())
                all_probs.extend(probs)
                total_eval_samples += len(batch_labels)
        total_inference_time = sum(inference_times)
        avg_inference_time_per_sample = total_inference_time / total_eval_samples if total_eval_samples > 0 else 0
        self.performance_tracker.add_inference_time(avg_inference_time_per_sample)
        end_eval_time = time.time()
        inference_time_ms = 1000 * (end_eval_time - start_eval_time) / total_eval_samples
        print(f"â± Inference Time: {inference_time_ms:.2f} ms/sample")


        f1 = f1_score(all_labels, all_predictions, average='weighted')
        accuracy = accuracy_score(all_labels, all_predictions)
        precision, recall, _ = precision_recall_curve(all_labels, all_probs)
        auc_pr = auc(recall, precision)
        auroc = roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else 0.0

        # REPLACE the return statement with:
        return {
            'f1': f1,
            'accuracy': accuracy,
            'auc_pr': auc_pr,
            'auroc': auroc,
            'conf_matrix': confusion_matrix(all_labels, all_predictions).tolist(),
            'class_report': classification_report(all_labels, all_predictions, output_dict=True),
            # Optional - comment these out for large datasets:
            'predictions': all_predictions if len(all_predictions) < 1000 else [],
            'labels': all_labels if len(all_labels) < 1000 else [],
            'probabilities': all_probs if len(all_probs) < 1000 else [],
            'inference_time_ms': inference_time_ms,
            'total_inference_time_ms': total_inference_time
        }
    

    def compute_oewc_fisher_and_params(self, dataloader):
        """Compute Fisher information and parameter snapshot for OEWC"""
        fisher = {}
        params = {}

        self.model.eval()
        for n, p in self.model.named_parameters():
            fisher[n] = torch.zeros_like(p)
            params[n] = p.detach().clone()

        criterion = FocalLoss(alpha=0.25, gamma=2.0)

        for batch_smiles, batch_labels in dataloader:
            batch_labels = batch_labels.float().to(self.device)
            if isinstance(batch_smiles, torch.Tensor):
                batch_smiles = batch_smiles.to(self.device)

            

            for n, p in self.model.named_parameters():
                if p.grad is not None:
                    fisher[n] += p.grad.detach().clone().pow(2)

        # Normalize fisher values
        for n in fisher:
            fisher[n] /= len(dataloader)

        return fisher, params


    def evaluate_forgetting(self, previous_task_loaders):
        """Evaluate catastrophic forgetting across previous tasks"""
        forgetting_metrics = {}

        for task_id, loader in previous_task_loaders.items():
            metrics = self.evaluate_task(loader, task_id)
            forgetting_metrics[task_id] = metrics

        return forgetting_metrics
    
def get_model_size_mb(model):
        """Calculate model size in MB"""
        param_size = 0
        buffer_size = 0
        
        for param in model.parameters():
            param_size += param.nelement() * param.element_size()
        
        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()
        
        model_size = (param_size + buffer_size) / (1024**2)  # Convert to MB
        return model_size


def upsample_dataset(smiles, labels):
    """Upsample minority class to balance dataset"""
    smiles = np.array(smiles)
    labels = np.array(labels)

    class0_idx = np.where(labels == 0)[0]
    class1_idx = np.where(labels == 1)[0]

    if len(class1_idx) == 0 or len(class1_idx) >= len(class0_idx):
        return smiles.tolist(), labels.tolist()

    class1_upsampled = resample(
        class1_idx,
        replace=True,
        n_samples=len(class0_idx),
        random_state=42
    )

    new_idx = np.concatenate([class0_idx, class1_upsampled])
    return smiles[new_idx].tolist(), labels[new_idx].tolist()


def create_sample_data():
    """Create sample molecular data for testing"""
    sample_smiles = [
        "CCO",  # Ethanol
        "CC(=O)O",  # Acetic acid
        "c1ccccc1",  # Benzene
        "CCN(CC)CC",  # Triethylamine
        "C1CCCCC1",  # Cyclohexane
        "CC(C)(C)O",  # tert-Butanol
        "CCCCO",  # Butanol
        "CC(C)O",  # Isopropanol
        "c1ccc(cc1)O",  # Phenol
        "CC(=O)N",  # Acetamide
    ]

    labels = [0, 1, 1, 0, 0, 1, 0, 1, 1, 0]

    return sample_smiles, labels

from torch.utils.data import Dataset, DataLoader
from sklearn.utils import resample

class MolecularDataset(Dataset):
    def __init__(self, smiles, labels, device=None):
        self.device = device
        self.smiles = smiles
        self.labels = labels

    def __len__(self):
        return len(self.smiles)

    def __getitem__(self, idx):
        return self.smiles[idx], torch.tensor(self.labels[idx]).to(self.device)

# Example usage
if __name__ == "__main__":
    import torch
    import matplotlib.pyplot as plt
    from torch.utils.data import DataLoader
    METHOD = "memo"

    print("Initializing ILR Model...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = ILRModel(binary_output=True)
    model.to(device)
    model.device = device
    trainer = ILRTrainer(model, method=METHOD, device=device)
    overall_start_time = time.time()
    print(f"\nðŸ” Verifying CL Method Setup...")
    trainer.validate_method_setup()
    if not trainer.validate_method_setup():
        print("âŒ Setup validation failed. Exiting.")
        exit(1)
    
    print(f"\nðŸŽ¯ Starting training with {METHOD.upper()} method")
    print("="*60)

    # Initialize results dictionary
    results = {
        'metadata': {
            'method': method,
            'start_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'device': str(device),
            'num_tasks': len(tasks),
            'seed': 42
        },
        'task_metrics': {},
        'performance_metrics': {
            'Q1_total_training_time_s': 0.0,
            'Q2_total_parameters': 0,
            'Q3_total_epochs': 0,
            'Q4_model_size_mb': 0.0,
            'Q5_throughput_samples_per_s': 0.0,
            'Q6_flops_millions': 0.0,
            'Q7_avg_inference_time_ms': 0.0
        },
        'task_accuracies': {},  # New: Store accuracies for each task
        'best_accuracies': {},  # New: Store best accuracy for each task
        'final_metrics': {
            'ainc': None,
            'average_best_accuracy': None,  # New: Average of all best accuracies
            'forgetting': {},
            'forward_transfer': {}
        }
    }
    # Calculate initial model parameters and size
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    model_size_mb = get_model_size_mb(model)
    
    results['performance_metrics']['Q2_total_parameters'] = total_params
    results['performance_metrics']['Q4_model_size_mb'] = model_size_mb

    print(f"Model parameters: {total_params:,} (trainable: {trainable_params:,})")
    print(f"Initial model size: {model_size_mb:.2f} MB")

    print("Creating sample data...")
    smiles_data, labels_data = create_sample_data()

    # Create dataset and dataloader
    dataset = SMILESDataset(smiles_data, labels_data)
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, drop_last=True, num_workers=0)

    print("Testing forward pass...")
    model.eval()
    for batch_smiles, batch_labels in dataloader:
        predictions, features, shared_features = model(batch_smiles, task_id=1)
        print(f"Batch size: {len(batch_smiles)}")
        print(f"Predictions shape: {predictions.shape}")
        print(f"Features shape: {features.shape}")
        print(f"Shared features shape: {shared_features.shape}")
        break

    print("\nILR Architecture successfully implemented!")
    print(f"Model parameters: {sum(p.numel() for p in model.parameters())}")

    # Tracking for evaluation metrics
    val_loaders = {}
    task_accuracies = {}
    task_best_accuracies = {}
    forward_transfer_scores = {}
    all_task_metrics = {task_id: [] for task_id in range(1, len(tasks)+1)}

    def get_batch_size(task_size):
        if task_size > 10000:
            return 64
        elif task_size > 5000:
            return 32
        elif task_size > 1000:
            return 16
        else:
            return 8

    for i, task in enumerate(tasks):
        task_id = i + 1
        task_name = task['name']
        task_size = len(task['train'][0])

        print(f"\n{'='*50}")
        print(f"Processing Task {task_id}/{len(tasks)}: {task_name}")
        print(f"Class balance: {task['class_balance']}")
        print(f"Training samples: {task_size}, Validation samples: {len(task['val'][0])}")
        
        batch_size = get_batch_size(task_size)
        print(f"Using batch size: {batch_size}")

        # Upsample and create datasets
        up_smiles, up_labels = upsample_dataset(task['train'][0], task['train'][1])
        train_dataset = MolecularDataset(up_smiles, up_labels, device=device)
        val_dataset = MolecularDataset(task['val'][0], task['val'][1], device=device)


        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True, num_workers=0)
        val_loader = DataLoader(val_dataset, batch_size=16, drop_last=False, num_workers=0)
        # Store validation loader
        val_loaders[task_id] = val_loader
        
        # Forward Transfer evaluation
        for future_task_id in range(task_id + 1, len(tasks) + 1):
            future_task = tasks[future_task_id - 1]
            future_val_dataset = MolecularDataset(future_task['val'][0], future_task['val'][1], device=device)
            future_val_loader = DataLoader(future_val_dataset, batch_size=16, drop_last=False, num_workers=0)

            ft_metrics = trainer.evaluate_task(future_val_loader, task_id=future_task_id)
            if future_task_id not in forward_transfer_scores:
                forward_transfer_scores[future_task_id] = []
            forward_transfer_scores[future_task_id].append(ft_metrics['accuracy'])
        
        if task_id > 1:
            # Initialize from previous rectifier if available
            if str(task_id-1) in model.rectifiers:
                model.hybrid_rectifier.load_state_dict(
                    model.rectifiers[str(task_id-1)].state_dict()
                )
                print(f"â™¨ï¸ Warm-started rectifier for task {task_id} from task {task_id-1}")


        print(f"ðŸ” Training Task {task_id} ({task_name})...")
        train_losses, val_metrics = trainer.train_task(
            train_loader=train_loader,
            val_loader=val_loader,
            val_loaders=val_loaders,  # ðŸŸ¡ This is the one you likely missed
            task_id=task_id,
            epochs=10
        )


        if task_id > 1:
            # Create alignment set (50% current task, 50% previous)
            align_set = model.create_alignment_set(
                current_task_data=train_loader.dataset,
                prev_task_data=val_loaders[task_id-1].dataset
            )
            align_loader = torch.utils.data.DataLoader(
                model.create_alignment_set(
                    current_task_data=train_loader.dataset,
                    use_generated=True
                ),
                batch_size=32,
                shuffle=True
            )
            
            # Train rectifiers
            trainer.train_rectifiers(align_loader, task_id)

        # Calculate final performance metrics
        overall_end_time = time.time()
        total_training_time = overall_end_time - overall_start_time

        # Store comprehensive task metrics
        best_accuracy_current_task = max([m['accuracy'] for m in val_metrics])
        results['task_metrics'][task_id] = {
            'name': task_name,
            'final_f1': float(val_metrics[-1]['f1']),
            'final_accuracy': float(val_metrics[-1]['accuracy']),
            'best_accuracy': float(best_accuracy_current_task),  # New: Best accuracy for this task
            'final_auc_pr': float(val_metrics[-1]['auc_pr']),
            'final_auroc': float(val_metrics[-1]['auroc']),
            'confusion_matrix': val_metrics[-1]['conf_matrix'],
            'class_balance': {
                '0': int(task['class_balance'][0]),
                '1': int(task['class_balance'][1])
            },
            'training_epochs': len(val_metrics),  # New: Number of epochs trained
            'task_size': task_size  # New: Training set size
        }
        
        # Store accuracy progression for this task
        results['task_accuracies'][task_id] = [float(m['accuracy']) for m in val_metrics]
        results['best_accuracies'][task_id] = float(best_accuracy_current_task)

        # Calculate and update performance metrics
        overall_end_time = time.time()
        total_training_time = overall_end_time - overall_start_time

        # Update performance metrics in results
        results['performance_metrics'].update({
            'Q1_total_training_time_s': float(total_training_time),
            'Q2_total_parameters': int(total_params),
            'Q3_total_epochs': int(trainer.performance_tracker.total_epochs),
            'Q4_model_size_mb': float(get_model_size_mb(model)),
            'Q5_throughput_samples_per_s': float(trainer.performance_tracker.get_throughput()),
            'Q6_flops_millions': float(trainer.model_flops) if trainer.model_flops else 0.0,
            'Q7_avg_inference_time_ms': float(trainer.performance_tracker.get_avg_inference_time())
        })


        # Print performance metrics
        print("\nðŸ” Performance Metrics Summary")
        print("="*50)
        print(f"Q1 - Total Training Time: {results['performance_metrics']['Q1_total_training_time_s']:.2f} seconds")
        print(f"Q2 - Total Parameters: {results['performance_metrics']['Q2_total_parameters']:,}")
        print(f"Q3 - Total Epochs: {results['performance_metrics']['Q3_total_epochs']}")
        print(f"Q4 - Model Size: {results['performance_metrics']['Q4_model_size_mb']:.2f} MB")
        print(f"Q5 - Throughput: {results['performance_metrics']['Q5_throughput_samples_per_s']:.2f} samples/s")
        print(f"Q6 - FLOPs: {results['performance_metrics']['Q6_flops_millions']:.2f} million")
        print(f"Q7 - Avg Inference Time: {results['performance_metrics']['Q7_avg_inference_time_ms']:.2f} ms")


        # Update tracking metrics
        all_task_metrics[task_id] = val_metrics
        if task_id not in task_accuracies:
            task_accuracies[task_id] = []
        task_accuracies[task_id].extend([m['accuracy'] for m in val_metrics])
        
        current_best = max([m['accuracy'] for m in val_metrics])
        if task_id not in task_best_accuracies or current_best > task_best_accuracies[task_id]:
            task_best_accuracies[task_id] = current_best

        # Print current task results
        print(f"âœ… Task {task_id} ({task_name}) F1: {val_metrics[-1]['f1']:.4f}, AUC-PR: {val_metrics[-1]['auc_pr']:.4f}, AUROC: {val_metrics[-1]['auroc']:.4f}")
        print("Confusion Matrix:\n", val_metrics[-1]['conf_matrix'])

        # Plotting
        val_f1s = [m['f1'] for m in val_metrics]
        plt.figure()
        plt.plot(train_losses, label="Train Loss")
        plt.plot(val_f1s, label="Val F1")
        plt.title(f"Task {task_id}: Loss vs F1")
        plt.legend()
        plt.savefig(f"task_{task_id}_loss_vs_f1.png")
        plt.close()

    # Calculate summary metrics after all tasks are completed
    current_accuracies = {t: all_task_metrics[t][-1]['accuracy'] for t in all_task_metrics if all_task_metrics[t]}
    ainc = sum(current_accuracies.values()) / len(current_accuracies) if current_accuracies else 0
    # Calculate average of best accuracies across all tasks
    best_accuracies_list = list(results['best_accuracies'].values())
    average_best_accuracy = sum(best_accuracies_list) / len(best_accuracies_list) if best_accuracies_list else 0
    
    # Add summary metrics to results
    results['final_metrics']['ainc'] = float(ainc)
    results['final_metrics']['average_best_accuracy'] = float(average_best_accuracy)
    
    # Forgetting metrics
    for t in sorted(task_best_accuracies.keys()):
        if t in current_accuracies:
            results['final_metrics']['forgetting'][t] = {
                'task_name': tasks[t-1]['name'],  # New: Include task name
                'best_accuracy': float(task_best_accuracies[t]),
                'final_accuracy': float(current_accuracies[t]),
                'forgetting': float(task_best_accuracies[t] - current_accuracies[t])
            }
    
    # Forward transfer metrics
    for tid in sorted(forward_transfer_scores.keys()):
        if forward_transfer_scores[tid]:
            results['final_metrics']['forward_transfer'][tid] = {
                'task_name': tasks[tid-1]['name'],  # New: Include task name
                'fam_score': float(sum(forward_transfer_scores[tid]) / len(forward_transfer_scores[tid])),
                'num_evaluations': len(forward_transfer_scores[tid])  # New: Number of evaluations
            }

    # Print final metrics
    print("\nðŸ” Final Evaluation Metrics Across All Tasks")
    print("="*60)
    print(f"AINC (Average Incremental Accuracy): {ainc:.4f}")
    print(f"Average Best Accuracy Across All Tasks: {average_best_accuracy:.4f}")

    print("\nBest Accuracy per Task:")
    for t in sorted(results['best_accuracies'].keys()):
        task_name = tasks[t-1]['name']
        print(f"  Task {t} ({task_name}): {results['best_accuracies'][t]:.4f}")
    
    print("Anytime Accuracy per Task:")
    for t in sorted(task_accuracies.keys()):
        if task_accuracies[t]:
            avg_acc = sum(task_accuracies[t]) / len(task_accuracies[t])
            task_name = tasks[t-1]['name']
            print(f"  Task {t} ({task_name}): {avg_acc:.4f}")

    print("Forgetting per Task:")
    for t in sorted(task_best_accuracies.keys()):
        if t in current_accuracies:
            forgetting = task_best_accuracies[t] - current_accuracies[t]
            task_name = tasks[t-1]['name']
            print(f"  Task {t} ({task_name}): {forgetting:.4f} (Best: {task_best_accuracies[t]:.4f}, Current: {current_accuracies[t]:.4f})")

    print("\nðŸ” Forward Transfer Measure (FAM):")
    for tid in sorted(forward_transfer_scores.keys()):
        if forward_transfer_scores[tid]:
            fam_score = sum(forward_transfer_scores[tid]) / len(forward_transfer_scores[tid])
            task_name = tasks[tid-1]['name']
            print(f"  Task {tid} ({task_name}): FAM = {fam_score:.4f}")

    # Save results to JSON
    def save_results(results, filename_prefix="ilr_results"):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        method_name = results['metadata']['method']
        filename = f"{filename_prefix}_{method}_{timestamp}.json"

        # Add final timestamp to metadata
        results['metadata']['end_time'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        results['metadata']['total_runtime_s'] = results['performance_metrics']['Q1_total_training_time_s']
        
        try:
            with open(filename, 'w') as f:
                json.dump(results, f, indent=4)
            print(f"\nâœ… Saved comprehensive results to {filename}")
            
            # Print JSON structure summary
            print("\nðŸ“‹ JSON Structure Summary:")
            print(f"  - Metadata: {len(results['metadata'])} fields")
            print(f"  - Task Metrics: {len(results['task_metrics'])} tasks")
            print(f"  - Performance Metrics: {len(results['performance_metrics'])} metrics")
            print(f"  - Best Accuracies: {len(results['best_accuracies'])} tasks")
            print(f"  - Final Metrics: AINC={results['final_metrics']['ainc']:.4f}, Avg Best Acc={results['final_metrics']['average_best_accuracy']:.4f}")
            
            return filename
        except Exception as e:
            print(f"\nâŒ Failed to save results: {str(e)}")
            return None

    saved_file = save_results(results)
    if saved_file:
        print(f"Results saved to: {os.path.abspath(saved_file)}")
    else:
        print("Failed to save results")

    # Final model info
    torch.save(model.state_dict(), "temp_model.pth")
    model_size_mb = os.path.getsize("temp_model.pth") / (1024 * 1024)
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6

    print(f"ðŸ§  Trainable Parameters: {trainable_params:.2f}M")
    print(f"ðŸ’¾ Model Size: {model_size_mb:.2f} MB")